{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJz2w03ETTiK4s7yrMFXKY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manasdeshpande125/da6401_assignment1/blob/main/DL_ASG1_Q3_Q10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "YS8zVkJ6UrVH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq9QECVKBZXT"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist,mnist\n",
        "import numpy as np\n",
        "from  matplotlib import pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Dataset**"
      ],
      "metadata": {
        "id": "Cpla6J2RUzcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= fashion_mnist.load_data()\n",
        "(X_train_and_validation, y_train_and_validation), (X_test, y_test) = dataset\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train_and_validation, y_train_and_validation, test_size=0.1, random_state=42)\n",
        "X_train = (X_train/255.0).astype(np.float32)\n",
        "X_validation = (X_validation/255.0).astype(np.float32)\n",
        "X_test = (X_test/255.0).astype(np.float32)\n",
        "\n",
        "print(\"Train Dataset Shape: \", X_train.shape)\n",
        "print(\"Train Target Vector Shape: \", y_train.shape)\n",
        "print(\"Test Dataset Shape:\", X_test.shape)\n",
        "print(\"Test Target Vector Shape\", y_test.shape)\n",
        "print(\"Validation Dataset Shape:\", X_validation.shape)\n",
        "print(\"Validation Target Vector Shape\", y_validation.shape)"
      ],
      "metadata": {
        "id": "tJ6M1BinBn3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train.reshape(X_train.shape[0], 784,1))\n",
        "X_test = np.array(X_test.reshape(X_test.shape[0], 784,1))\n",
        "X_validation = np.array(X_validation.reshape(X_validation.shape[0], 784,1))"
      ],
      "metadata": {
        "id": "qIYsnpjUBq7o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization and Activation Functions**"
      ],
      "metadata": {
        "id": "B2QvSP-jU978"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(arr,n1,n2,init_type):\n",
        "    np.random.seed(10)\n",
        "    if init_type==\"random\":\n",
        "        arr.append(np.random.randn(n1,n2)*0.1)\n",
        "    elif init_type==\"xavier\":\n",
        "        arr.append(np.random.randn(n1,n2)*np.sqrt(2/(n1+n2)))\n",
        "    return arr\n",
        "\n",
        "def param(num_input_nodes, num_hidden_layers, hidden_layer_size, out_num, init_type):\n",
        "    W = []\n",
        "    B = []\n",
        "\n",
        "    layers = [num_input_nodes]  # Input layer\n",
        "    layers.extend([hidden_layer_size] * num_hidden_layers)  # Dynamic hidden layers\n",
        "    layers.append(out_num)  # Output layer\n",
        "\n",
        "    for i in range(len(layers) - 1):\n",
        "        W = layer_init(W, layers[i + 1], layers[i], init_type)\n",
        "        B = layer_init(B, layers[i + 1], 1, init_type)\n",
        "\n",
        "    return W, B\n",
        "\n",
        "#Activation function\n",
        "def activation(activation_function):\n",
        "    if activation_function == 'sigmoid':\n",
        "        return sigmoid\n",
        "    if activation_function == 'tanh':\n",
        "        return tanh\n",
        "    if activation_function == 'ReLU':\n",
        "        return relu\n",
        "    if activation_function == 'identity':\n",
        "        return identity\n",
        "\n",
        "\n",
        "def sigmoid(x, derivative = False):\n",
        "    if derivative:\n",
        "        return sigmoid(x)*(1-sigmoid(x))\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def tanh(x, derivative = False):\n",
        "    if derivative:\n",
        "        return 1 - tanh(x)**2\n",
        "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "\n",
        "def relu(x, derivative = False):\n",
        "    if derivative:\n",
        "        return (x>0)*1\n",
        "    return x*(x>0)\n",
        "\n",
        "def softmax(x,derivative = False):\n",
        "    if derivative:\n",
        "        return softmax(x)*(1- softmax(x))\n",
        "    return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
        "\n",
        "def identity(x,derivative=False):\n",
        "  if derivative:\n",
        "        return np.ones_like(x)\n",
        "  return x\n",
        "\n",
        "def softmax1(x,derivative = False):\n",
        "    if derivative:\n",
        "        return softmax1(x)*(1- softmax1(x))\n",
        "    x = np.array(x)\n",
        "    x -= np.max(x, axis=0, keepdims=True)  # Normalize values to avoid large exponentials\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / (np.sum(exp_x, axis=0, keepdims=True) + 1e-10)  # Prevent divide by zero\n",
        "\n",
        "def one_hot(y, num_output_nodes):\n",
        "    v = np.zeros((num_output_nodes, len(y)))\n",
        "    for i,j in enumerate(y):\n",
        "        v[j,i] = 1\n",
        "    return v\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "adIVaesNBtld"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feed_Forward Function**"
      ],
      "metadata": {
        "id": "BNlH9RxrVLRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x, W, B, activation_type):\n",
        "    h = []\n",
        "    a = []\n",
        "    sigma = activation(activation_type)  #activation\n",
        "    h.append(x)   #h0 = x\n",
        "    a.append(np.dot(W[0], h[0]) + B[0])\n",
        "    for i in range(len(W)-1):\n",
        "        h.append(sigma(a[-1]))\n",
        "        a.append(np.dot(W[i+1], h[-1]) + B[i+1])\n",
        "    y_hat = softmax1(a[-1])\n",
        "\n",
        "    return y_hat, h, a"
      ],
      "metadata": {
        "id": "3F_WOr1pVIyr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function**"
      ],
      "metadata": {
        "id": "Df4yY7aFVQVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y,y_hat,l_type,W,reg,n_class):\n",
        "    if l_type=='cross_entropy':\n",
        "        err = -1 * np.sum(np.multiply(one_hot(y, n_class), np.log(np.clip(y_hat, 1e-10, 1.0)))) / one_hot(y, n_class).shape[1]\n",
        "    elif l_type=='squared_error':\n",
        "        err=np.sum((one_hot(y,n_class)-y_hat)**2)/(2*one_hot(y,n_class)).shape[1]\n",
        "\n",
        "    if W:\n",
        "        r=0\n",
        "        for i in range(len(W)):\n",
        "            r+=np.sum((np.array(W,dtype=object)**2)[i])\n",
        "        err=err+reg*r\n",
        "    return err\n",
        "\n",
        "def eval_acc(y_hat, y_true):\n",
        "    return np.mean(np.argmax(y_hat, axis = 0) ==y_true )*100\n"
      ],
      "metadata": {
        "id": "zZHV0L6iBz72"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back_Propagation Function**"
      ],
      "metadata": {
        "id": "gE2dj-6pVUFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(x, y, y_hat, a, h , W, B, batch_size,l_type,act_type):\n",
        "    grad_h,grad_a,grad_W,grad_B = [0]*len(h),[0]*len(a),[0]*len(W),[0]*len(B)\n",
        "    sigma = activation(act_type)\n",
        "\n",
        "    if l_type == \"cross_entropy\":\n",
        "        grad_h[-1] = -1 * (y / (y_hat + 1e-10))\n",
        "        grad_a[-1] = -1*(y-y_hat)\n",
        "    if l_type == \"squared_error\":\n",
        "        grad_h[-1] = -2 * (y - y_hat)  # Correct derivative of squared error loss\n",
        "        grad_a[-1] = grad_h[-1] * (y_hat * (1 - y_hat))  # Chain rule for activation function\n",
        "\n",
        "    for i in range(len(W)-1, -1, -1):\n",
        "        grad_W[i] = np.dot(grad_a[i], h[i].T)\n",
        "        #grad_B[i] = np.dot(grad_a[i], np.ones((batch_size,1)))\n",
        "        grad_B[i] = np.sum(grad_a[i], axis=1, keepdims=True)\n",
        "        if i > 0:\n",
        "            grad_h[i-1] = np.dot(W[i].T, grad_a[i])\n",
        "            grad_a[i-1]  = np.multiply(grad_h[i-1],sigma(a[i-1], derivative = True))\n",
        "\n",
        "    return grad_W, grad_B, grad_h, grad_a"
      ],
      "metadata": {
        "id": "Ndc-GA6YB4AO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normal Sigmoid Gradient Descent**"
      ],
      "metadata": {
        "id": "RbBh8aCBVb2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_step(W, B, grad_W, grad_B, lr, reg):\n",
        "    # Convert lists to numpy arrays\n",
        "    W = [w - lr * reg * w - lr * gw for w, gw in zip(W, grad_W)]\n",
        "    B = [b - lr * reg * b - lr * gb for b, gb in zip(B, grad_B)]\n",
        "\n",
        "    return W, B\n"
      ],
      "metadata": {
        "id": "om1Oqq9hDLuR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Momentum Gradient Descent**"
      ],
      "metadata": {
        "id": "vJ6AWrRGVkI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def momentum_step(w, b, gW, gB, lr, moment, reg):\n",
        "    params = {'w': w, 'b': b}\n",
        "\n",
        "    # Initialize momentum buffers for weights and biases\n",
        "    Wmoments = [np.zeros_like(wi) for wi in params['w']]\n",
        "    Bmoments = [np.zeros_like(bi) for bi in params['b']]\n",
        "    print(Wmoments)\n",
        "    # Update momentum buffers\n",
        "    Wmoments = [moment * wm + lr * gw for wm, gw in zip(Wmoments, gW)]\n",
        "    Bmoments = [moment * bm + lr * gb for bm, gb in zip(Bmoments, gB)]\n",
        "\n",
        "    # Update weights and biases\n",
        "    W = [(1 - lr * reg) * wi - wm for wi, wm in zip(params['w'], Wmoments)]\n",
        "    B = [(1 - lr * reg) * bi - bm for bi, bm in zip(params['b'], Bmoments)]\n",
        "\n",
        "    return W, B\n",
        "\n",
        "momentum_step([1,2,3,4],[1,1,1,1],[1,1,1,1],[1,1,1,1],1,1,1)\n"
      ],
      "metadata": {
        "id": "Xr6JCKEjDOkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec0b9d6-a699-4d8c-f592-aadcaf63c857"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(0), array(0), array(0), array(0)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([-1, -1, -1, -1], [-1, -1, -1, -1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMS Gradient Descent**"
      ],
      "metadata": {
        "id": "7-9fAl7SVn7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSprop_step(w, b, gW, gB, lr, beta, epsilon=1e-7,reg=0):\n",
        "    params = {'w': w, 'b': b}\n",
        "\n",
        "    # Initialize moving average of squared gradients\n",
        "    vW = [np.zeros_like(wi) for wi in params['w']]\n",
        "    vB = [np.zeros_like(bi) for bi in params['b']]\n",
        "\n",
        "    # Update moving averages of squared gradients\n",
        "    vW = [beta * vw + (1 - beta) * (gw ** 2) for vw, gw in zip(vW, gW)]\n",
        "    vB = [beta * vb + (1 - beta) * (gb ** 2) for vb, gb in zip(vB, gB)]\n",
        "\n",
        "    # Update parameters\n",
        "    W = [wi - (lr / np.sqrt(vw + epsilon)) * gw for wi, vw, gw in zip(params['w'], vW, gW)]\n",
        "    B = [bi - (lr / np.sqrt(vb + epsilon)) * gb for bi, vb, gb in zip(params['b'], vB, gB)]\n",
        "\n",
        "    return W, B\n"
      ],
      "metadata": {
        "id": "d6KjETJzDXJi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nesterov Gradient Descent**"
      ],
      "metadata": {
        "id": "aeKikhPVVsqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_sgd_step(w, b, gW, gB, lr, moment, reg=0):\n",
        "    params = {'w': w, 'b': b}\n",
        "\n",
        "    # Initialize momentum terms\n",
        "    Wmoments = [np.zeros_like(wi) for wi in params['w']]\n",
        "    Bmoments = [np.zeros_like(bi) for bi in params['b']]\n",
        "\n",
        "    # Lookahead step: Apply momentum before computing gradients\n",
        "    lookahead_W = [wi - moment * wm for wi, wm in zip(params['w'], Wmoments)]\n",
        "    lookahead_B = [bi - moment * bm for bi, bm in zip(params['b'], Bmoments)]\n",
        "\n",
        "    # Compute updated momentum\n",
        "    Wmoments = [moment * wm + lr * gw for wm, gw in zip(Wmoments, gW)]\n",
        "    Bmoments = [moment * bm + lr * gb for bm, gb in zip(Bmoments, gB)]\n",
        "\n",
        "    # Apply weight decay regularization and update parameters\n",
        "    W = [(1 - lr * reg) * wi - wm for wi, wm in zip(lookahead_W, Wmoments)]\n",
        "    B = [(1 - lr * reg) * bi - bm for bi, bm in zip(lookahead_B, Bmoments)]\n",
        "\n",
        "    return W, B\n",
        "\n"
      ],
      "metadata": {
        "id": "x971WbL9DaOv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adam's Gradient Descent**"
      ],
      "metadata": {
        "id": "p2aBwQysVxhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam_sgd_step(w, b, gW, gB, lr, beta1, beta2, epsilon, reg=0, t=1):\n",
        "    params = {'w': w, 'b': b}\n",
        "\n",
        "    # Initialize moment estimates as lists of zero arrays for each layer\n",
        "    Wm = [np.zeros_like(wi) for wi in params['w']]\n",
        "    Wv = [np.zeros_like(wi) for wi in params['w']]\n",
        "    Bm = [np.zeros_like(bi) for bi in params['b']]\n",
        "    Bv = [np.zeros_like(bi) for bi in params['b']]\n",
        "\n",
        "    # Convert gradients to NumPy arrays\n",
        "    gW = [np.array(gi) for gi in gW]\n",
        "    gB = [np.array(gi) for gi in gB]\n",
        "\n",
        "    # Update biased first moment estimate\n",
        "    Wm = [beta1 * wm + (1 - beta1) * gw for wm, gw in zip(Wm, gW)]\n",
        "    Bm = [beta1 * bm + (1 - beta1) * gb for bm, gb in zip(Bm, gB)]\n",
        "\n",
        "    # Update biased second raw moment estimate\n",
        "    Wv = [beta2 * wv + (1 - beta2) * (gw ** 2) for wv, gw in zip(Wv, gW)]\n",
        "    Bv = [beta2 * bv + (1 - beta2) * (gb ** 2) for bv, gb in zip(Bv, gB)]\n",
        "\n",
        "    # Compute bias-corrected moment estimates\n",
        "    Wm_hat = [wm / (1 - beta1 ** t) for wm in Wm]\n",
        "    Wv_hat = [wv / (1 - beta2 ** t) for wv in Wv]\n",
        "    Bm_hat = [bm / (1 - beta1 ** t) for bm in Bm]\n",
        "    Bv_hat = [bv / (1 - beta2 ** t) for bv in Bv]\n",
        "\n",
        "    # Update parameters\n",
        "    W = [(1 - lr * reg) * wi - lr * (wm_h / (np.sqrt(wv_h) + epsilon)) for wi, wm_h, wv_h in zip(params['w'], Wm_hat, Wv_hat)]\n",
        "    B = [(1 - lr * reg) * bi - lr * (bm_h / (np.sqrt(bv_h) + epsilon)) for bi, bm_h, bv_h in zip(params['b'], Bm_hat, Bv_hat)]\n",
        "\n",
        "    return W, B\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MOeGmxnPDxDD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nesterov's Accelerated Adam**"
      ],
      "metadata": {
        "id": "k6-sClDvWHDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam_sgd_step(w, b, gW, gB, lr, beta1, beta2, epsilon, reg=0, t=1):\n",
        "    params = {'w': w, 'b': b}\n",
        "\n",
        "    # Initialize moment estimates properly\n",
        "    Wm = [np.zeros_like(wi) for wi in params['w']]\n",
        "    Wv = [np.zeros_like(wi) for wi in params['w']]\n",
        "    Bm = [np.zeros_like(bi) for bi in params['b']]\n",
        "    Bv = [np.zeros_like(bi) for bi in params['b']]\n",
        "\n",
        "    # Convert gradients to NumPy arrays\n",
        "    gW = [np.array(gi) for gi in gW]\n",
        "    gB = [np.array(gi) for gi in gB]\n",
        "\n",
        "    # Compute lookahead momentum term for Nesterov-like update\n",
        "    Wm = [beta1 * wm + (1 - beta1) * gw for wm, gw in zip(Wm, gW)]\n",
        "    Bm = [beta1 * bm + (1 - beta1) * gb for bm, gb in zip(Bm, gB)]\n",
        "\n",
        "    Wm_nesterov = [beta1 * wm + (1 - beta1) * gw for wm, gw in zip(Wm, gW)]\n",
        "    Bm_nesterov = [beta1 * bm + (1 - beta1) * gb for bm, gb in zip(Bm, gB)]\n",
        "\n",
        "    # Update biased second raw moment estimate\n",
        "    Wv = [beta2 * wv + (1 - beta2) * (gw ** 2) for wv, gw in zip(Wv, gW)]\n",
        "    Bv = [beta2 * bv + (1 - beta2) * (gb ** 2) for bv, gb in zip(Bv, gB)]\n",
        "\n",
        "    # Compute bias-corrected moment estimates\n",
        "    Wm_hat = [wm_n / (1 - beta1 ** t) for wm_n in Wm_nesterov]\n",
        "    Wv_hat = [wv / (1 - beta2 ** t) for wv in Wv]\n",
        "    Bm_hat = [bm_n / (1 - beta1 ** t) for bm_n in Bm_nesterov]\n",
        "    Bv_hat = [bv / (1 - beta2 ** t) for bv in Bv]\n",
        "\n",
        "    # Update parameters\n",
        "    W = [(1 - lr * reg) * wi - lr * (wm_h / (np.sqrt(wv_h) + epsilon)) for wi, wm_h, wv_h in zip(params['w'], Wm_hat, Wv_hat)]\n",
        "    B = [(1 - lr * reg) * bi - lr * (bm_h / (np.sqrt(bv_h) + epsilon)) for bi, bm_h, bv_h in zip(params['b'], Bm_hat, Bv_hat)]\n",
        "\n",
        "    return W, B\n",
        "\n"
      ],
      "metadata": {
        "id": "QOXRmae-D_sE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Function**"
      ],
      "metadata": {
        "id": "UuiVjt4kWM_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train, y_train, x_val, y_val, num_inputs_nodes, num_hidden_layers, hidden_layer_size, out_num, init_type, epochs,\n",
        "          batch_size, l_type, act_type, op_name, lr_rate, m=0.5,weight_decay=0, beta=0.5, beta1=0.5, beta2=0.5, epsilon=0.000001):\n",
        "\n",
        "    # Initialize weights and biases dynamically\n",
        "    W, B = param(num_inputs_nodes, num_hidden_layers, hidden_layer_size, out_num, init_type)\n",
        "\n",
        "    N = X_train.shape[0]\n",
        "    n_batches = int(np.floor(N / batch_size))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = []\n",
        "        train_accuracy = []\n",
        "        val_loss = []\n",
        "        val_accuracy = []\n",
        "        l, acc, temp, ds, steps = 0, 0, 0, 0, 1\n",
        "\n",
        "        while ds < N:\n",
        "            mini_batch_size = min((N - ds), batch_size)\n",
        "            x = np.squeeze(X_train[ds:ds + mini_batch_size]).T\n",
        "            y = one_hot(y_train[ds:ds + mini_batch_size], out_num)\n",
        "            y_hat, h, a = forward(x, W, B, act_type)\n",
        "            grad_W, grad_B, grad_h, grad_a = back_prop(x, y, y_hat, a, h, W, B, batch_size, l_type, act_type)\n",
        "\n",
        "            # Choose optimizer dynamically\n",
        "            if op_name == 'sgd':\n",
        "                W, B = sgd_step(W, B, grad_W, grad_B, lr_rate, weight_decay)\n",
        "            elif op_name == 'momentum':\n",
        "                W, B = momentum_step(W, B, grad_W, grad_B, lr_rate, m, weight_decay)\n",
        "            elif op_name == 'rmsprop':\n",
        "                W, B = RMSprop_step(W, B, grad_W, grad_B, lr_rate, beta=beta, reg=weight_decay)\n",
        "            elif op_name == \"nesterov\":\n",
        "                W, B = nesterov_sgd_step(W, B, grad_W, grad_B, lr_rate, m, weight_decay)\n",
        "            elif op_name == \"adam\":\n",
        "                W, B = adam_sgd_step(W, B, grad_W, grad_B, lr=lr_rate, beta1=beta1, beta2=beta2, epsilon=epsilon, reg=weight_decay,t=steps)\n",
        "            elif op_name == \"nadam\":\n",
        "                W, B = nadam_sgd_step(W, B, grad_W, grad_B, lr=lr_rate, beta1=beta1, beta2=beta2, epsilon=epsilon, reg=weight_decay,t=steps)\n",
        "\n",
        "            l += loss(y_train[ds:ds + mini_batch_size], y_hat, l_type, W, weight_decay, out_num)\n",
        "            acc += eval_acc(y_hat, y_train[ds:ds + mini_batch_size])\n",
        "            steps += 1\n",
        "            ds += batch_size\n",
        "\n",
        "        #Normalizing\n",
        "        l /= (n_batches + mini_batch_size)\n",
        "        acc /= steps\n",
        "\n",
        "        #appending history of all loss and accuracy\n",
        "        train_loss.append(l)\n",
        "        train_accuracy.append(acc)\n",
        "\n",
        "        y_val_hat, _, _ = forward(np.squeeze(x_val).T, W, B, act_type)\n",
        "        val_acc = eval_acc(y_val_hat, y_val)\n",
        "        val_l = loss(y_val, y_val_hat, l_type, W=None, reg=weight_decay, n_class=out_num)\n",
        "        val_accuracy.append(val_acc)\n",
        "        val_loss.append(val_l)\n",
        "\n",
        "        wandb.log({\"epoch\": epoch, \"Train_loss\": l, \"Train_acc\": acc, \"val_loss\": val_l, \"val_Accuracy\": val_acc})\n",
        "\n",
        "    return W, B, train_loss, train_accuracy, val_loss, val_accuracy\n"
      ],
      "metadata": {
        "id": "uCgGPCHpIMuY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Sweep Configuration**"
      ],
      "metadata": {
        "id": "VytAVJtNWRV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_configuration = {\n",
        "    'method': \"bayes\",\n",
        "    'metric': {'name': 'val_Accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'epochs': {'values': [5, 10]},\n",
        "        'num_hidden_layers': {'values': [3, 4, 5]},  # New parameter for no. of hidden layers\n",
        "        'hidden_layer_size': {'values': [32, 64, 128]},  # Define possible layer sizes\n",
        "        'learning_rate': {'values': [1e-3, 1e-4]},\n",
        "        'weight_decay': {'values': [0, 0.0005, 0.5]},\n",
        "        'optimizer_name': {'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']},\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'init_type': {'values': ['random', 'xavier']},\n",
        "        'activation_type': {'values': ['sigmoid', 'tanh', 'ReLU']},\n",
        "        'loss_type': {'values': ['cross_entropy']}\n",
        "        #'squared_error', 'identity'\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "oGKUr7ElJYqW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Wandb's Sweep Method for checking Hyperparameters**"
      ],
      "metadata": {
        "id": "pQ0teJywWwoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sweep_train():\n",
        "    wandb.init(project=\"DA6401-Assignment-1\", entity=\"manasdeshpande4902-iit-madras\", config=sweep_configuration)\n",
        "    config = wandb.config\n",
        "\n",
        "    # Retrieve parameters from sweep config\n",
        "    epochs = config.epochs\n",
        "    num_hidden_layers = config.num_hidden_layers\n",
        "    hidden_layer_size = config.hidden_layer_size\n",
        "    learning_rate = config.learning_rate\n",
        "    weight_decay = config.weight_decay\n",
        "    optimizer_name = config.optimizer_name\n",
        "    batch_size = config.batch_size\n",
        "    init_type = config.init_type\n",
        "    activation_type = config.activation_type\n",
        "    loss_type = config.loss_type\n",
        "    wandb.run.name = \"e_{}_hl_{}_lr_{}_wd_{}_o_{}_bs_{}_winit_{}_ac_{}_los_{}\".format(epochs,\\\n",
        "                                                                                    num_hidden_layers,\\\n",
        "                                                                                    learning_rate,\\\n",
        "                                                                                    weight_decay,\\\n",
        "                                                                                    optimizer_name,\\\n",
        "                                                                                    batch_size,\\\n",
        "                                                                                    init_type,\\\n",
        "                                                                                    activation_type,\\\n",
        "                                                                                    loss_type)\n",
        "    W, B, train_loss, train_accuracy, val_loss, val_accuracy = train(\n",
        "        X_train, y_train, X_validation, y_validation,\n",
        "        784, num_hidden_layers, hidden_layer_size, 10, init_type, epochs,\n",
        "        batch_size, loss_type, activation_type, optimizer_name, learning_rate,weight_decay=weight_decay\n",
        "    )\n"
      ],
      "metadata": {
        "id": "r1TcyFTSKhA9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_configuration,project='DA6401-Assignment-1')\n",
        "wandb.agent(sweep_id,function=sweep_train,project='DA6401-Assignment-1',count=150)"
      ],
      "metadata": {
        "id": "TMBeFHDHLJ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Type1 Confusion Matrix**"
      ],
      "metadata": {
        "id": "4PpsnQwSWZjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(id=\"confusion_matrix3\",project=\"DA6401-Assignment-1\")\n",
        "# Train model and get predictions\n",
        "W_new, B_new, train_loss, train_accuracy, val_loss, val_accuracy = train(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        784, 4, 64, 10, \"xavier\", 10,\n",
        "        32, \"cross_entropy\", \"ReLU\", \"adam\", 0.001,weight_decay=0.005\n",
        "    )\n",
        "Y, _, _ = forward(np.squeeze(X_test).T, W_new, B_new, \"ReLU\")\n",
        "Y_prediction = np.argmax(Y, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "labels_dict_names =  [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "wandb.log({\"Confusion matrix\": wandb.plot.confusion_matrix(probs=None,y_true=y_test,preds=Y_prediction,class_names=labels_dict_names)})"
      ],
      "metadata": {
        "id": "DWqH9wjSSVbK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Type2 Confusion Matrix**"
      ],
      "metadata": {
        "id": "LNAdUXRYWf_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "wandb.init(id=\"confusion_matrix2\", project=\"DA6401-Assignment-1\")\n",
        "\n",
        "# Train model and get predictions\n",
        "W_new, B_new, train_loss, train_accuracy, val_loss, val_accuracy = train(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        784, 4, 64, 10, \"xavier\", 10,\n",
        "        32, \"cross_entropy\", \"ReLU\", \"adam\", 0.001,weight_decay=0.005\n",
        "    )\n",
        "Y, _, _ = forward(np.squeeze(X_test).T, W_new, B_new, \"ReLU\")\n",
        "Y_prediction = np.argmax(Y, axis=0)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, Y_prediction)\n",
        "\n",
        "# Plot using Seaborn\n",
        "labels_dict_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels_dict_names, yticklabels=labels_dict_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "# Save figure and log to WandB\n",
        "conf_matrix_img = \"confusion_matrix.png\"\n",
        "plt.savefig(conf_matrix_img)\n",
        "wandb.log({\"Confusion Matrix\": wandb.Image(conf_matrix_img)})\n",
        "\n",
        "# Close the plot\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "3ZiC4jkcnCWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Type3 Confusion Matrix**"
      ],
      "metadata": {
        "id": "OrmqlqCSP3u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from wandb.sklearn import plot_confusion_matrix\n",
        "\n",
        "wandb.init(id=\"confusion_matrix4\", project=\"DA6401-Assignment-1\")\n",
        "\n",
        "# Train model and get predictions\n",
        "W_new, B_new, train_loss, train_accuracy, val_loss, val_accuracy = train(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        784, 4, 64, 10, \"xavier\", 10,\n",
        "        32, \"cross_entropy\", \"ReLU\", \"adam\", 0.001,weight_decay=0.005\n",
        "    )\n",
        "Y, _, _ = forward(np.squeeze(X_test).T, W_new, B_new, \"ReLU\")\n",
        "Y_prediction = np.argmax(Y, axis=0)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, Y_prediction)\n",
        "\n",
        "# Plot using Seaborn\n",
        "labels_dict_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "wandb.sklearn.plot_confusion_matrix(y_test, Y_prediction, labels=labels_dict_names)"
      ],
      "metadata": {
        "id": "rNP57v2hKZc5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}